---
title: "Задачи по Эконометрике-2: Качество подгонки и сравнение моделей"
author: "Н.В. Артамонов (МГИМО МИД России)"
output:
  html_document:
    toc: true
    toc_float: 
        collapsed: false
    number_sections: true
    df_print: paged
---

```{r, message=FALSE, echo=FALSE}
library(stargazer)
library(lmtest)
library(car)
library(sandwich)
library(jtools)
library(DescTools)
data(loanapp, package = 'wooldridge')
data(SwissLabor, package = 'AER')
mroz_Greene <- read.csv('http://www.stern.nyu.edu/~wgreene/Text/Edition7/TableF5-1.csv')
```

# Качество подгонки

## labour force equation #1 (probit)

Для датасета [mroz_Greene](http://www.stern.nyu.edu/~wgreene/Text/Edition7/TableF5-1.csv)
рассморим несколько probit-регрессй. Результаты оценивания

```{r, echo=FALSE, message=FALSE, warning=FALSE, comment=''}
my.digits <- 3
my.digits.output <- 4
sign.level <- 0.10
regr <- NULL
regr[[1]] <- glm(formula=LFP~WA+I(WA^2)+WE+KL6+K618+CIT+UN+log(FAMINC), data=mroz_Greene,
                 family = binomial(link='probit') )
regr[[2]] <- update(regr[[1]], formula. = ~.-WA-I(WA^2) )
regr[[3]] <- update(regr[[1]], formula. = ~.-CIT-UN-K618 )
regr[[4]] <- update(regr[[1]], formula. = ~.-CIT-UN-K618-log(FAMINC) )
stargazer(regr, type='text', digits=my.digits.output, digit.separator = '', 
          dep.var.caption = 'Зависимая переменная', df=FALSE)

n <- nobs(regr[[1]])

my.logL <- NULL
my.k <- NULL
my.Efron <- NULL
my.McKelvey <- NULL
for(i in 1:length(regr)) {
  my.logL <- c(my.logL, -deviance(regr[[i]])/2 )
  my.k <- c(my.k, regr[[i]]$rank-1)
  efron <- 1-sum( (regr[[i]]$y-regr[[i]]$fitted.values)^2 )/sum( (regr[[i]]$y-mean(regr[[i]]$y))^2 )
  mckelvey <- (var(regr[[i]]$fitted.values)*(n-1))/(var(regr[[i]]$fitted.values)*(n-1)+n)
  my.Efron <- c(my.Efron, efron)
  my.McKelvey <- c(my.McKelvey, mckelvey)
}
rm(efron)
my.logL.null <- -regr[[1]]$null.deviance/2
```

Для каждой регрессии вычислитк следующие показатели качества подгонки модели: \(pseudoR^2\), 
\(pseudoR^2_adj\), Cox & Snell, Nagelkerke/Cragg & Uhler, Efron, McKelvey & Zavoina. **Ответ округлите до `r my.digits`-х десятичных знаков.**

Ответ: 

```{r, echo=FALSE, comment=''}
goodness.of.fit <- data.frame(Model=1:length(regr),
                              pseudoR2=1-my.logL/my.logL.null,
                              pseudoR.2adj=1-(my.logL-my.k)/my.logL.null,
                              CoxSnell=1-(exp(2*(my.logL.null-my.logL)/n)),
                              Nagelkerke=(1-(exp(2*(my.logL.null-my.logL)/n)))/(1-exp(2*my.logL.null/n)),
                              Efron = my.Efron,
                              McKelveyZavoina = my.McKelvey
                              )
stargazer(goodness.of.fit, type='text', summary=FALSE, rownames=FALSE, digits = my.digits)
```

## approve equation #1 (logit)

Для датасета `loanapp`
рассморим несколько logit-регрессй. Результаты оценивания

```{r, echo=FALSE, message=FALSE, warning=FALSE, comment=''}
my.digits <- 3
my.digits.output <- 4
sign.level <- 0.01
regr <- NULL
regr[[1]] <- glm(formula=approve~appinc+I(appinc^2)+mortno+unem+dep+male+married+yjob+self, data=loanapp,
                 family = binomial(link='logit') )
regr[[2]] <- update(regr[[1]], formula. = ~.-appinc-I(appinc^2) )
regr[[3]] <- update(regr[[1]], formula. = ~.-male-yjob-self )
regr[[4]] <- update(regr[[1]], formula. = ~.-male-yjob-self-unem-married )
stargazer(regr, type='text', digits=my.digits.output, digit.separator = '', 
          dep.var.caption = 'Зависимая переменная', df=FALSE)
n <- nobs(regr[[1]])

my.logL <- NULL
my.k <- NULL
my.Efron <- NULL
my.McKelvey <- NULL
for(i in 1:length(regr)) {
  my.logL <- c(my.logL, -deviance(regr[[i]])/2 )
  my.k <- c(my.k, regr[[i]]$rank-1)
  efron <- 1-sum( (regr[[i]]$y-regr[[i]]$fitted.values)^2 )/sum( (regr[[i]]$y-mean(regr[[i]]$y))^2 )
  mckelvey <- (var(regr[[i]]$fitted.values)*(n-1))/(var(regr[[i]]$fitted.values)*(n-1)+n*pi^2/3)
  my.Efron <- c(my.Efron, efron)
  my.McKelvey <- c(my.McKelvey, mckelvey)
}
rm(efron)
my.logL.null <- -regr[[1]]$null.deviance/2
```

Для каждой регрессии вычислитк следующие показатели качества подгонки модели: \(pseudoR^2\), 
\(pseudoR^2_adj\), Cox & Snell, Nagelkerke/Cragg & Uhler, Efron, McKelvey & Zavoina. **Ответ округлите до `r my.digits`-х десятичных знаков.**

Ответ: 

```{r, echo=FALSE, comment=''}
goodness.of.fit <- data.frame(Model=1:length(regr),
                              pseudoR2=1-my.logL/my.logL.null,
                              pseudoR.2adj=1-(my.logL-my.k)/my.logL.null,
                              CoxSnell=1-(exp(2*(my.logL.null-my.logL)/n)),
                              Nagelkerke=(1-(exp(2*(my.logL.null-my.logL)/n)))/(1-exp(2*my.logL.null/n)),
                              Efron = my.Efron,
                              McKelveyZavoina = my.McKelvey
                              )
stargazer(goodness.of.fit, type='text', summary=FALSE, rownames=FALSE, digits = my.digits)
```

# Сравнение моделей

## labour force equation #1 (probit)

Для датасета [mroz_Greene](http://www.stern.nyu.edu/~wgreene/Text/Edition7/TableF5-1.csv)
рассморим несколько probit-регрессй. Результаты оценивания

```{r, echo=FALSE, message=FALSE, warning=FALSE, comment=''}
my.digits <- 3
my.digits.output <- 4
sign.level <- 0.10
regr <- NULL
regr[[1]] <- glm(formula=LFP~WA+I(WA^2)+WE+KL6+K618+CIT+UN+log(FAMINC), data=mroz_Greene,
                 family = binomial(link='probit') )
regr[[2]] <- update(regr[[1]], formula. = ~.-WA-I(WA^2) )
regr[[3]] <- update(regr[[1]], formula. = ~.-CIT-UN-K618 )
regr[[4]] <- update(regr[[1]], formula. = ~.-CIT-UN-K618-log(FAMINC) )

for(i in 1:length(regr) ) {
  # regr[[i]]$AIC <- AIC(regr[[i]])
  regr[[i]]$BIC <- BIC(regr[[i]])
}

my.logL.null <- -regr[[1]]$null.deviance/2
n <- nobs(regr[[1]])

stargazer(regr, type='text', digits=my.digits.output, digit.separator = '', keep.stat = c('n', 'll'),
          dep.var.caption = 'Зависимая переменная', df=FALSE)
```

Для каждой модели вычислите показатели информационнвх критериев AIC & BIC и \(pseudoR^2_adj\). **Ответ округлите до `r my.digits`-х десятичных знаков.**

Ответ

```{r, echo=FALSE, comment=''}
my.AIC <- NULL
my.BIC <- NULL
my.logL <- NULL
my.k <- NULL
for(i in 1:length(regr)) {
  my.AIC <- c(my.AIC, regr[[i]]$aic)
  my.BIC <- c(my.BIC, regr[[i]]$BIC)
  my.logL <- c(my.logL, -deviance(regr[[i]])/2 )
  my.k <- c(my.k, regr[[i]]$rank-1)
}

info.criterion <- data.frame(Модель=1:length(regr), AIC=my.AIC, BIC=my.BIC, pseudoR.2adj=1-(my.logL-my.k)/my.logL.null)

stargazer(info.criterion, type='text', summary = FALSE, rownames = FALSE, digit.separator = '', digits=my.digits)
```

Какая модель предпочтительней по информационных критериям и \(pseudoR^2_adj\)?

Ответ: 

```{r, echo=FALSE, comment=''}
df <- data.frame(Критерий=c('AIC', 'BIC', 'pseudoR.2adj'),
                      Регрессия=c(which.min(my.AIC), which.min(my.BIC), which.max(info.criterion$pseudoR.2adj) ) )
stargazer(df, type='text', summary = FALSE, rownames = FALSE)
```

## approve equation #1 (logit)

Для датасета `loanapp`
рассморим несколько probit-регрессй. Результаты оценивания

```{r, echo=FALSE, message=FALSE, warning=FALSE, comment=''}
my.digits <- 3
my.digits.output <- 4
sign.level <- 0.10
regr <- NULL
regr[[1]] <- glm(formula=approve~appinc+I(appinc^2)+mortno+unem+dep+male+married+yjob+self, data=loanapp,
                 family = binomial(link='probit') )
regr[[2]] <- update(regr[[1]], formula. = ~.-appinc-I(appinc^2) )
regr[[3]] <- update(regr[[1]], formula. = ~.-male-yjob-self )
regr[[4]] <- update(regr[[1]], formula. = ~.-male-yjob-self-unem-married )

for(i in 1:length(regr) ) {
  # regr[[i]]$AIC <- AIC(regr[[i]])
  regr[[i]]$BIC <- BIC(regr[[i]])
}

my.logL.null <- -regr[[1]]$null.deviance/2
n <- nobs(regr[[1]])

stargazer(regr, type='text', digits=my.digits.output, digit.separator = '', keep.stat = c('n', 'll'),
          dep.var.caption = 'Зависимая переменная', df=FALSE)
```

Для каждой модели вычислите показатели информационнвх критериев AIC & BIC и \(pseudoR^2_adj\). **Ответ округлите до `r my.digits`-х десятичных знаков.**

Ответ

```{r, echo=FALSE, comment=''}
my.AIC <- NULL
my.BIC <- NULL
my.logL <- NULL
my.k <- NULL
for(i in 1:length(regr)) {
  my.AIC <- c(my.AIC, regr[[i]]$aic)
  my.BIC <- c(my.BIC, regr[[i]]$BIC)
  my.logL <- c(my.logL, -deviance(regr[[i]])/2 )
  my.k <- c(my.k, regr[[i]]$rank-1)
}

info.criterion <- data.frame(Модель=1:length(regr), AIC=my.AIC, BIC=my.BIC, pseudoR.2adj=1-(my.logL-my.k)/my.logL.null)

stargazer(info.criterion, type='text', summary = FALSE, rownames = FALSE, digit.separator = '', digits=my.digits)
```

Какая модель предпочтительней по информационных критериям и \(pseudoR^2_adj\)?

Ответ: 

```{r, echo=FALSE, comment=''}
df <- data.frame(Критерий=c('AIC', 'BIC', 'pseudoR.2adj'),
                      Регрессия=c(which.min(my.AIC), which.min(my.BIC), which.max(info.criterion$pseudoR.2adj) ) )
stargazer(df, type='text', summary = FALSE, rownames = FALSE)
```