{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод наименьших квадратов\n",
    "\n",
    "Рассмотрим переменные:\n",
    "\n",
    "* $y$ – зависимая переменная, отклик, __target__  (количественная!)\n",
    "* $\\begin{pmatrix} x_1 &\\cdots & x_k \\end{pmatrix}$ – объясняющие/влияющие переменные, регрессоры, __features__ (количественные/категориальные)\n",
    "\n",
    "для которых имеем серию $n$ наблюдений: \n",
    "\n",
    "$$\\{y_i, x_{i1},\\ldots, x_{ik} \\}_{i=1}^n$$\n",
    "\n",
    "## Задача прогнозирования\n",
    "\n",
    "Рассмотрим функцию/модель $a(x_1,\\ldots,x_k)$, которую будем использовать для прогнозирования $y$ при заданных \n",
    "$\\begin{pmatrix} x_1 &\\cdots & x_k \\end{pmatrix}$.\n",
    "\n",
    "Обычно модель зависит то нескольких параметров $a(x_1,\\ldots,x_k)=a(x_1,\\ldots,x_k,\\beta)$.\n",
    "\n",
    "Прогноз обозначим $\\hat{y}=a(x_1,\\ldots,x_k)=a(x_1,\\ldots,x_k,\\beta)$\n",
    "\n",
    "__Задача__: на данных обучить выбранную модель, чтобы использовать её для прогнозирования на новых наблюдениях.\n",
    "\n",
    "Что нужно?\n",
    "\n",
    "1. Выбрать модель $a(x_1,\\ldots,x_k)$\n",
    "2. Выбрать функцию потерь $L(y,\\hat{y})$, показывающую точность прогнозирования\n",
    "\n",
    "Подгонка модели: решаем задачу оптимизации\n",
    "\n",
    "$$\n",
    "\t\\min_{\\beta} \\frac{1}{n}\\sum_{i=1}^n L(y_i,\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Параметры подогнанной модели обозначим $\\hat{\\beta}$\n",
    "\n",
    "## Метод наименьших квадратов (OLS) для линейной регрессии\n",
    "\n",
    "В качестве модели для прогнозирования выборем __линейную регрессию__ (с константой)\n",
    "\n",
    "$$\n",
    "\ta(x_1,\\ldots,x_k)=\\beta_0+\\beta_1x_1+\\cdots+\\beta_kx_k\n",
    "$$\n",
    "\n",
    "Функция потерь $L(y,\\hat{y})=|y-\\hat{y}|^2$ (квадрат ошибки прогнозирования для наблюдения).  Она соответствует метрике MSE = Mean Squared Error\n",
    "\n",
    "Для удобства обозначим \n",
    "* $\\beta=\\begin{pmatrix} \\beta_0 & \\beta_1 & \\cdots & \\beta_k \\end{pmatrix}^\\top$\n",
    "* $x=\\begin{pmatrix} 1 & x_1 & \\cdots & x_k \\end{pmatrix}^\\top$\n",
    "\n",
    "Тогда $a(x_1,\\ldots,x_k)=x^\\top \\beta=\\langle x, \\beta\\rangle$\n",
    "\n",
    "Подгонка модели (выбор коэффициентов): решаем задачу оптимизации\n",
    "\n",
    "$$\n",
    "\t\\min_{\\beta} \\frac{1}{n}\\sum_{i=1}^n |y_i-\\hat{y}_i|^2\n",
    "$$\n",
    "\n",
    "или\n",
    "\n",
    "$$\n",
    "\t\\min_{\\beta} \\frac{1}{n}\\sum_{i=1}^n |y_i-\\beta_0-\\beta_1x_{i1}-\\cdots-\\beta_kx_{ik}|^2\n",
    "$$\n",
    "\n",
    "## Метод наименьших квадратов для линейной регрессии 2D\n",
    "\n",
    "Рассмотрим случай одного предиктора, т.е. $a(x)=\\beta_0+\\beta_1x$ (геометрически прямая на плоскости $(x,y)$). \n",
    "\n",
    "Обозначим (сумма квадратов ошибок прогнозов)\n",
    "\n",
    "$$\n",
    "\tRSS=\\sum_{i=1}^n |y_i-\\hat{y}_i|^2=\\sum_{i=1}^n (y_i-\\beta_0-\\beta_1 x_i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "Коэффициенты подогнанной модели находятся как решение задачи (безусловной) оптимизации\n",
    "\n",
    "$$\n",
    "\t\\min_{\\beta_0,\\beta_1}\\frac{1}{n}RSS\n",
    "$$\n",
    "(на плоскости подгоняем прямую под наблюдения)\n",
    "\n",
    "Необходимые условия\n",
    "\n",
    "$$\n",
    "\t\\left\\{\\begin{aligned} \n",
    "\t\t\\frac{\\partial}{\\partial \\beta_0}\\left(\\frac{1}{n}RSS\\right) &=0 \\\\ \n",
    "\t\t\\frac{\\partial}{\\partial \\beta_1}\\left(\\frac{1}{n}RSS\\right) &=0\n",
    "\t\\end{aligned}\\right.\n",
    "$$\n",
    "\n",
    "Имеем\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t\\frac{\\partial}{\\partial \\beta_0}\\left(\\frac{1}{n}RSS\\right) &=\\frac{1}{n}\\sum_{i=1}^n 2(y_i-\\beta_0-\\beta_1 x_i)(-1)=\n",
    "\t(-2)\\left(\\frac{1}{n}\\sum_{i=1}^n y_i-\\frac{1}{n}\\sum_{i=1}^n \\beta_0-\\frac{1}{n}\\sum_{i=1}^n \\beta_ix_i\\right)=\n",
    "\t(-2)(\\bar{y}-\\beta_0-\\beta_1 \\bar{x})\\\\\n",
    "\t\\frac{\\partial}{\\partial \\beta_1}\\left(\\frac{1}{n}RSS\\right) &=\\frac{1}{n}\\sum_{i=1}^n 2(y_i-\\beta_0-\\beta_1 x_i)(-x_i)=\n",
    "\t(-2)\\left(\\frac{1}{n}\\sum_{i=1}^n x_iy_i-\\frac{1}{n}\\sum_{i=1}^n \\beta_0x_i-\\frac{1}{n}\\sum_{i=1}^n \\beta_ix_i^2\\right)=\n",
    "\t(-2)(\\overline{xy}-\\beta_0\\bar{x}-\\beta_1\\overline{x^2})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Получаем систему\n",
    "\n",
    "$$\n",
    "\t\\left\\{\\begin{aligned} \n",
    "\t\t\\beta_0+\\beta_1\\bar{x} &=\\bar{y} \\\\\n",
    "\t\t\\beta_0\\bar{x}+\\beta_1\\overline{x^2} &= \\overline{xy}\n",
    "\t\\end{aligned}\\right.\n",
    "$$\n",
    "\n",
    "Решение системы \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t\\hat{\\beta}_1 &= \\frac{\\overline{xy}-\\bar{x}\\cdot\\bar{y}}{\\overline{x^2}-\\left(\\bar{x}\\right)^2}=\n",
    "\t\\frac{cov(x,y)}{Var(x)} & \\hat{\\beta}_0&=\\bar{y}-\\hat{\\beta}_1\\cdot\\bar{x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "__Замечание__: оптимальная прямая $\\hat{y}=\\hat{a}(x)=\\hat{\\beta}_0+\\hat{\\beta}_1x$ проходит через \"центр масс\" \n",
    "наблюдений $(\\bar{x}, \\bar{y})$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
